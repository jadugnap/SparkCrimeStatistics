{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"crime_id\", StringType(), True),\n",
    "    StructField(\"original_crime_type_name\", StringType(), True),\n",
    "    StructField(\"report_date\", StringType(), True),\n",
    "    StructField(\"call_date\", StringType(), True),\n",
    "    StructField(\"offense_date\", StringType(), True),\n",
    "    StructField(\"call_time\", StringType(), True),\n",
    "    StructField(\"call_date_time\", StringType(), True),\n",
    "    StructField(\"disposition\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"agency_id\", StringType(), True),\n",
    "    StructField(\"address_type\", StringType(), True),\n",
    "    StructField(\"common_location\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark variable is undefined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(spark)\n",
    "except:\n",
    "    print(\"spark variable is undefined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f68a0123c18>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"KafkaSparkStructuredStreaming\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "logger.info(\"Spark started\")\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO Create Spark Configuration\n",
    "# Create Spark configurations with max offset of 200 per trigger\n",
    "# set up correct bootstrap server and port\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"maxOffsetsPerTrigger\", 200) \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"com.crime.police.calls\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"stopGracefullyOnShutdown\", \"true\") \\\n",
    "        .load()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO extract the correct column from the kafka input resources\n",
    "# Take only value and convert it to String\n",
    "kafka_df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- crime_id: string (nullable = true)\n",
      " |-- original_crime_type_name: string (nullable = true)\n",
      " |-- report_date: string (nullable = true)\n",
      " |-- call_date: string (nullable = true)\n",
      " |-- offense_date: string (nullable = true)\n",
      " |-- call_time: string (nullable = true)\n",
      " |-- call_date_time: string (nullable = true)\n",
      " |-- disposition: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- agency_id: string (nullable = true)\n",
      " |-- address_type: string (nullable = true)\n",
      " |-- common_location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_table = kafka_df\\\n",
    "    .select(psf.from_json(psf.col('value'), schema).alias(\"CALLS_TOPICS\"))\\\n",
    "    .select(\"CALLS_TOPICS.*\")\n",
    "service_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- original_crime_type_name: string (nullable = true)\n",
      " |-- disposition: string (nullable = true)\n",
      " |-- call_date_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO select original_crime_type_name and disposition\n",
    "distinct_table = service_table\\\n",
    "    .selectExpr( \\\n",
    "        \"original_crime_type_name\", \\\n",
    "        \"disposition\", \\\n",
    "        \"to_timestamp(call_date_time) as call_date_time\" \\\n",
    "    ) \\\n",
    "    .withWatermark(\"call_date_time\", \"60 minutes\")\n",
    "distinct_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- original_crime_type_name: string (nullable = true)\n",
      " |-- disposition: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count the number of original crime type\n",
    "agg_df = distinct_table\\\n",
    "    .withWatermark(\"call_date_time\", \"60 minutes\") \\\n",
    "    .groupBy( \\\n",
    "        psf.window(distinct_table.call_date_time, \"60 minutes\", \"10 minutes\"), \\\n",
    "        distinct_table.original_crime_type_name, \\\n",
    "        distinct_table.disposition \\\n",
    "    ).count()\n",
    "agg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.streaming.StreamingQuery object at 0x7f68a0133978>\n"
     ]
    }
   ],
   "source": [
    "# TODO Q1. Submit a screen shot of a batch ingestion of the aggregation\n",
    "# TODO write output stream\n",
    "query = agg_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "print(query)\n",
    "\n",
    "# TODO attach a ProgressReporter\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- disposition: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO get the right radio code json path\n",
    "# TODO rename disposition_code column to disposition\n",
    "radio_code_json_filepath = \"radio_code.json\"\n",
    "radio_code_df = spark.read.json(radio_code_json_filepath, multiLine=True).withColumnRenamed(\"disposition_code\", \"disposition\")\n",
    "radio_code_df.printSchema()\n",
    "\n",
    "# clean up your data so that the column names match on radio_code_df and agg_df\n",
    "# we will want to join on the disposition code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- disposition: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- original_crime_type_name: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO join on disposition column\n",
    "join_df = agg_df.join(radio_code_df, \"disposition\")\n",
    "join_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.streaming.StreamingQuery object at 0x7f68a0123eb8>\n"
     ]
    }
   ],
   "source": [
    "join_query = join_df\\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"join\") \\\n",
    "    .start()\n",
    "print(join_query)\n",
    "# join_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark stopped\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"spark stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
